<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Statistical power, p-values and effect sizes</title>
    <meta charset="utf-8" />
    <meta name="author" content="Daniel Hammarström" />
    <meta name="date" content="2020-10-30" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Statistical power, <em>p</em>-values and effect sizes
### Daniel Hammarström
### IDR4000
### 2020-10-30

---




---

# Null hypothesis significance testing (NHST)

--

- NHST is the most common way of making *decisions* about **effects** within the sport sciences.

--

- NHST can be used to assess if e.g. groups are different or regression parameters are different than zero.

--

- NHST can be performed using the following steps:

--

1. Choose a *null*-hypothesis, e.g. there is no differences between groups `\(H_0:\mu_1 = \mu_2\)`, and a alternative hypothesis e.g. `\(H_1:\mu_1 - \mu_2 \neq 0\)`
2. Specify a **significance level**, usually 5% (or `\(\alpha=0.05\)`).
3. Perform an appropriate test, in the case of differences between means, a *t* test and calculate the `\(p\)`-value 
4. If the `\(p\)`-value is less than the stated `\(\alpha\)`-level we declare the result as statistically significant and reject `\(H_0\)`.

---

# NHST is a special flavour of hypothesis testing

- Two competing views on hypothesis testing were originally presented by Ronald A. Fisher on the one hand and Jerzy Neyman and Egon Pearson on the other hand.


|Fisher     |Neyman-Pearson     |
|-----------|-------------------|
|1. State `\(H_0\)` | 1. State `\(H_0\)` and `\(H_1\)`                                              |  
|2. Specify test statistic  | 2. Specify `\(\alpha\)` (e.g. 5%)   |                            
|3. Collect data, calculate test statistic and `\(p\)`-value | 3. Specify test statistics and critical value            |       
|4. Reject `\(H_0\)` if `\(p\)` is small | 4. Collect data, calculate test statistic, determine `\(p\)`  |
|   | 5. Reject `\(H_0\)` if favor of `\(H_1\)` if `\(p &lt; \alpha\)`|


&lt;font size = "3"&gt;
Kline, R. B. (2013). *Beyond significance testing: Statistics reform in the behavioral sciences*, 2nd ed. Washington, DC, US, American Psychological Association
&lt;/font&gt;

---

# The `\(p\)`-value

--

- The `\(p\)`-value is the probability of obtaining a value of a **test statistic** (t) as extreme as the one obtained or more extreme under the condition that the null-hypothesis is true:

`$$p(t|H_0)$$`

--

- We assume that the **null is true** and we calculate how often a results such as the one obtained would occur as a result of chance. However, using `\(\alpha = 0.05\)` we simply declare *significant* when `\(p&lt;\alpha\)` (and accept that we will be wrong in 5% of repeated studies), **this is the Neyman-Person approach**, 

--

- The `\(\alpha\)`-level is the Type 1 error rate, the probability of rejecting `\(H_0\)` when it is actually true.

---

# Interpreting `\(p\)`-values

--

- There are two distinct ways of looking at the `\(p\)`-value, one where the `\(p\)`-value is a pre-specified threshold for decision (Neyman-Pearson), and one where the `\(p\)`-value is thought of as a **meassure of strength of evidence** against the null-hypothesis (Fisher).

--

- It is common practice to combine the two approaches in analysis of scientific experiments. Examples:

--

  * "There was not a significant difference between groups but the p-values suggested a trend towards ..."
  * "The difference between group A and B was significant, but the difference between A and C was highly significant"
--

- According to the original frameworks, the mix (Fisher combined with Neyman-Pearson) leads to abuse of NHST

---

# More on *p*-value interpretation
&lt;center&gt;
&lt;img src=https://wp-media.patheos.com/blogs/sites/617/2017/10/pvaluepumpkin.jpg width = 50%&gt;
&lt;/center&gt;
&lt;font size = 3&gt;Twitter: @AcademicsSay&lt;/font&gt;


---

# Statistical power

--
- Neyman and Pearson extended Fishers hypothesis testing procedure with the concept of power.
--

- An alternative hypothesis can be stated for a specific value of e.g. a difference `\(H_1: \mu_1-\mu_2 = 5\)`
--

- Using this alternative hypothesis we can calculate the statistical power: The probability of **rejecting** `\(H_0\)` if the alternative hypothesis is true. 
--

- The probability of failing to reject `\(H_0\)` if `\(H_1\)` is true is the Type 2 error rate `\(\beta\)`.
--

- Statistical power is therefore: `\(1-\beta\)`.

---
# Errors in NHST

- There are two scenarios where we make mistakes, by rejecting `\(H_0\)` when it is actually true and not rejecting `\(H_0\)` when it is false.


|              | Accept H&lt;sub&gt;0&lt;/sub&gt; | Reject H&lt;sub&gt;0&lt;/sub&gt; |
|--------------|--------------|--------------|
|**H&lt;sub&gt;0&lt;/sub&gt; is true** |Correct!      |Type I error  |
|**H&lt;sub&gt;0&lt;/sub&gt; is false**|Type II error |Correct!      |

---

# Error rates in NHST

- We usually specify the level of Type I errors to 5%
--

- Another convention is to specify the power to 80%, this means that the risk of **failing to reject** `\(H_0\)` when `\(H_0\)` is **false** is 20%.
--

- These levels are chosen by tradition(!), but a well designed study is planned using well thought through Type I and II error rates.
--

- In the case of `\(\alpha = 0.05\)` and `\(\beta=0.2\)`, Cohen (1988) pointed out that this can be thought of as Type I errors being a mistake four times more serious than Type II errors. 

`$$\frac{0.20}{0.05} = 4$$`
--

- Rates could be adjusted to represent the relative seriousness of respective errors.

---

# The `\(\alpha\)`-error and statistical power are related

&lt;img src="lecture-2020-2-pvalues_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

---

# Error rates in NHST, an example

--

- If a study tries to determine if a novel treatment with no known side-effects should be implemented, **failure to detect a difference** compared to placebo when **there is a difference** (Type II error) would be more serious than to detect a difference that is not true (Type I error).

--

- In this case error rates could be adjusted to reflect this, decrease possibility Type II errors by increasing the possibility of Type I errors.

---

# Power analysis in NHST

- When planning a study within human exercise physiology, we want to know *how many participants to recruit. *

--

- This is a question of **cost** as more participants means **more work**

--

- It is a question of **ethics** as more participants means that more people are subjected to risk/discomfort. 

--

- We aim to recruit as many participants as is necessary to answer our question.

--

- We state our `\(H_0\)` and `\(H_1\)` (according to the Neyman-Pearson tradition).

--

- The `\(H_1\)` has a special function, this can be seen as the smallest meaningful difference between conditions under study, the difference we want to be able to detect.

--

- When we have specified `\(H_1\)` we can perform power analysis and **sample size estimation**.

---

# Power analysis, an example

- We want to compare the muscle mass gains as a result of two resistance training protocols.

--

- A 1 kg difference in lean mass increases is considered a meaningful difference after 12 weeks of training.

--

- The standard deviation from previous studies is used to estimate the expected variation in responses to 12 weeks of resistance training.  

--

- We plan to perform our experiment with equal sized groups and assume they will have the same variation ( `\(\sigma = 2.5\)` ).

--

- To calculate the required sample size we first must calculate a standardized effect size, also known as Cohen's `\(d\)`. 

--

- We can standardize our "effect size" of 1 kg by dividing by the SD.
`$$d = \frac{1}{2.5}= 0.4$$`
---
# Effect sizes

- The effect size is the primary aim of an experiment, we wish to know the difference, correlation, regression coefficient, percentage change... 

--

- The effect size can be standardized (e.g. divided by the standard deviation or calculated as e.g. a correlation).


---

# Power analysis, an example cont.

--

- We must specify `\(\alpha\)` and `\(1-\beta\)` to calculate the required sample size, let's say that the Type I error is four times more serious than the Type II error, and that we would accept to be wrong in rejecting `\(H_0\)` at a rate of 5%.

`$$\alpha=0.05, \beta=0.2, d = 0.4$$`
--



```r
power &lt;- pwr.t.test(d = 0.4, power = 0.8,
                         sig.level = 0.05, type = "two.sample", 
                         alternative = "two.sided")
```


- Given these specifics we would require 100 in each group to be able to show a meaningful difference with the power set to 80%.

--

- This is a **big study** what if we examine the smallest difference we can detect using a set sample size

--

---

# Mean difference between groups vs. number of participants per group

&lt;img src="lecture-2020-2-pvalues_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---

# Power analysis

Statistical power is influenced by:


- The `\(\alpha\)` level

--

- The direction of the hypothesis (negative, positive or both ways different from `\(H_0\)`)

--

- Experimental design (within- or between-participants)

--

- The statistical test

--

- Reliability of test scores

---
# Power analysis in R

--

- The `pwr` package can be used to calculate sample size, power, effect sizes or `\(\alpha\)`-levels.

--

- The relationship between these values can be used to calculate one unknown. 

--

- We simply must "guess" values for some of the values using data from previous studies



```r
pwr.t.test(power = 0.8, 
           d = 0.4,
           sig.level = 0.05, 
           type = "two.sample",
           alternative = "two.sided")
```

---

# Critique of NHST

--

- NHST with `\(p\)`-values tend to create an "either-or" situation, gives no answer about the size of an effect

--

- Test statistics are related to sample size, small effects can be detected using big sample sizes

--

- Built into the NHST framework is the acceptance of a proportion of tests being false positive, the likelihood of getting false positives increases with the number of tests.

---


# Statistical significance and clinical significance

--

Large sample sizes can make small effect sizes statistically significant. Example, Lee, I-Min et al (2010):  

--

- Objective: To examine the association of different amounts of physical activity with long-term weight changes among women consuming a usual diet.

--

- Design: Prospective cohort study, following 34,079 healthy, US women (mean age, 54.2 years) from 1992–2007. At baseline, 36-, 72-, 96-, 120-, 144- and 156-months’ follow-up, women reported their physical activity and body weight.

--

- Results: Women gained a mean of 2.6 kg throughout the study.

|Comparison |Mean difference |*p*-value|
|-----------|----------------|---------|
|7.5 - &lt;21 MET vs. `\(\geq\)` 21 MET | 0.11 kg | 0.003|
|&lt; 7.5 MET vs. `\(\geq\)` 21 MET |0.12 kg | 0.002| 



---

# Results 

&lt;img src="img/weightgain.jpg" width="80%" style="display: block; margin: auto;" /&gt;


&lt;font size = "3"&gt;
*Lee, I-Min et al. "Physical Activity and Weight Gain Prevention" JAMA: the journal of the American Medical Association 303.12 (2010): 1173–1179. PMC. Web. 24 Sept. 2018.
&lt;/font&gt;
---

# Making the wrong decision 5% of the time

--

- Given that NHST accepts mistakes at a rate of `\(\alpha\)`, e.g. every `\(\frac{1}{\alpha_{0.05}}=20^{th}\)` test result will be false. 

--

- The Neyman-Pearson approach is to only do NHST with an pre-specified `\(\alpha\)`-level

--

- One must also avoid making up hypotheses after the test.

--

- If you do multiple tests, family-wise corrections can be made, e.g. the Bonferroni correction:

`$$\alpha_{Bonferroni}=\frac{\alpha}{n~tests}$$`
--

- For statistical significance to be reached, the `\(\alpha_{Bonferroni}\)` threshold must be reached.

---

# Summary

--

- The `\(p\)`-value is the probability of the observed test result (or a more extreme) when the null hypothesis `\((H_0)\)` is true

--

- `\(p\)`-values can be seen as a threshold for decision about `\(H_0\)`, or the degree of evidence against `\(H_0\)`.

--

- As `\(p\)`-values are related to sample size, null-hypothesis testing should be performed in studies where sample sizes are selected based on a power analysis. 

--
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
