<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Statistical Inference 1</title>
    <meta charset="utf-8" />
    <meta name="author" content="Daniel Hammarström" />
    <meta name="date" content="2020-10-21" />
    <script src="libs/header-attrs-2.4/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Statistical Inference 1
### Daniel Hammarström
### IDR4000
### 2020-10-21

---

class: center, middle
# Statistical inference

What are Rønnestad and Vikmoen talking about here?

&lt;img src="./img/ronnestad2019_example.png" width="100%" /&gt;

--
Specifically, are they talking about the results in their study, or the "real world" it represents?


---
# Statistical inference

- A scientific study is often designed to answer questions about the "real world" based on some collected data. 

--

- This is because we are unable to measure all possible cases.

--

- The data are collected under controlled circumstances so that claims about the "real world" are unbiased

--

- Researchers aim to describe e.g. a difference between two training protocols outside the laboratory,  given some set of assumptions and based on a sample of data.

--

- Based on our collected data, we want to _**infer**_ what the _**true**_ real-world value is.


---

# Population and sample

- The population (in statistical terms) are all the **possible values** a **variable** can take.

--

- We can therefore talk about a population of values, e.g. height of all men in Norway, or all possible test results of a group of athletes.

--

- The sample is a **subset** of values from the population.

--

- The problem we face when trying to infer about the *population* based on a *sample* is that we will estimate with some *error*.

--

- If we have designed our experiment in a bad way, the error will to a large extent consist of **bias**.

--

- If the experiment is well designed, without bias, we are still left with **sampling error**


---

# A simple example


&lt;img src="./img/container.jpg" width="40%" style="display: block; margin: auto;" /&gt;

--

- We order blue and red pills from China. We messed up the order and the factory forgot to document how many blue pills there were. 

--

- We need to know the proportion of blue pills as blue and red pills has different prices.

--

- The total number of pills in our containers is `\(N = 10000000\)`, we wont be able to count them all, we can only use a sample. 

---

# A simple example (simulations)


```r
set.seed(123)

N &lt;- 10000000 # Number of pills
# The actual proportions
pills &lt;- c(rep("blue", 0.4 * N ), rep("red", 0.6 * N)) 

# Sample pills
*small_sample1 &lt;- sample(pills, 10, replace = FALSE)
*small_sample2 &lt;- sample(pills, 10, replace = FALSE)
```

--

- We open a box and count 10 pills, there are 4 blue and 6 red pills (40% blue).

--

- Another 10 pills are drawn there are 3 blue and 7 red pills (30% blue).

--

- As long as all containers/boxes are unbiased, the differences in samples are due to sampling error.

--

- Your boss tells you that you only need to count 50 pills to know the true proportions of pills.

--

- You think it makes sense that if the sample size is bigger we are more confident that the sample represents the actual population.
- To prove you point, you spend the night counting pills...

---

# A distribution of samples

--

- When we draw a sample from a population, it will give us a best guess of the true proportion.

--

- The true, unknown population proportion can be called `\(p\)`, our best guess can be called `\(\hat{p}\)`

--

- When we calculate the proportion of blue pills in a sample it might be `\(\hat{p}=0.45\)` (45%). In another draw, it might be `\(\hat{p}=0.55\)`. 

--

- If we would have drawn many samples we would have created a **sampling distribution**.

--

- To prove to your boss that sample sizes matters, you draw random samples of 25, 50 and 100 and count the number of blue pills. 

---

# A distribution of samples

&lt;img src="lecture-2020-1-inference1_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---

# A sampling distribution can be modeled using the normal distribution

&lt;img src="lecture-2020-1-inference1_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---

# The sampling distribution of proportions

--

- You have spent all night counting pills and proven to your boss that the sample size matters in the sense that bigger samples more often gives a value close to your best guess. 

--

- It turned out that the sampling distribution looked like a normal distribution created with the the `\(p\)` as center and a larger spread with smaller sample sizes.

--

- After reading up on some statistics, you find out that the spread or standard deviation of the sampling distribution for proportions could be calculated as 

`$$= \sqrt \frac{p(1-p)} {n}$$`

--

- This is called the **standard error**.

--

- If we only take one sample, **we can estimate** the mean and spread of the sampling distribution

---

# The normal distribution

- Given some assumptions, a sample can be used to estimate a sampling distribution.

--

- This estimated sampling distribution is then used to draw inference about the real-world.

--

- A property of the normal distribution is that `\(95\%\)` of the data fits under `\(\pm  1.96 \times\)` the standard deviation

--

- As the standard deviation of the sampling distribution can be estimated using the standard error (taking sample size into account), we can estimate a range a plausible values of the sampling distribution

---

![](lecture-2020-1-inference1_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

---
# The confidence interval

- A confidence interval contains `\(95\%\)` of the data of the estimated sampling distribution. 

--

- The confidence interval thus has an interpretation: **upon repeated sampling, the 95% confidence interval will contain the true value 95% of the time**

--

- To prove this point, we can make a small simulation: (1) Using the &lt;span style="color:blue"&gt;blue&lt;/span&gt; and &lt;span style="color:red"&gt;red&lt;/span&gt; pills, draw a sample and calculate `\(\hat{p}\)` and a `\(95\%\)` confidence interval. (2) repeat the process to check how many intervals contains the true value (0.4).
 
---

# Code for simulation


```r
set.seed(101)
results.ci &lt;- data.frame(phat = rep(NA, 1000), 
                         cil = rep(NA, 1000), 
                         ciu = rep(NA, 1000))

*for(i in 1:1000){
  
 phat &lt;- table(sample(pills, 100))[1]/100
  
  results.ci[i, 1] &lt;- phat
  results.ci[i, 2] &lt;- phat - (1.96 * sqrt((phat * (1-phat))/100))
  results.ci[i, 3] &lt;- phat + (1.96 * sqrt((phat * (1-phat))/100))
  
}
```


---


&lt;img src="lecture-2020-1-inference1_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

---

# The confidence interval for hypothesis testing

--

- The confidence interval can be used to perform hypothesis testing as the interval contains plausible values of true parameter.

--

- A statistical hypothesis test consists of to competing positions of denoted as `\(H_0\)`, the null-hypothesis and `\(H_A\)`, the alternative hypothesis. 

--

- &lt;span style="color:red"&gt;By convention, we test against `\(H_0\)`. Why?&lt;/span&gt;

---

# The confidence interval for hypothesis testing

--

- Let's say that if everything is done at random, our Chinese factory will produce equally many blue and red pills. 

--

- This can be our `\(H_0\)`: There is no difference in the number of blue and red pills ( `\(p=0.5\)` ).

--

- An alternative hypothesis could be that `\(H_A\)`: Manufacturing is biased in some direction, `\(p\neq 0.5\)`.

--

- We can use the confidence interval to test these hypotheses. As we would test against the `\(H_0\)` we would create confidence intervals a check if they contained the `\(H_0\)` or not.



&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Sample size &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(\hat{p}\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CI lower bound &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CI upper bound &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 25 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.400 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.208 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.592 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.380 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.248 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.512 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 100 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.490 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.384 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.596 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.409 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.378 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.440 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


---

# Continuous data: Population and sample

--

- Up to now, we have used proportions as examples. 

--

- The same principles are relevant for other types of data such continuous data.

--

- When we use data on the interval or ratio scale, the sample mean is used to estimate the population parameter (the population mean).

The population mean:

`$$\mu=\frac{\sum{X_i}}{N}$$`

The sample mean:

`$$\bar{x}=\frac{\sum{x_i}}{n}$$`

---

The mean is a measure of central tendency, *example*:
`$$X_1 = 90, X_2 = 95, X_3 = 100, X_4 = 105, X_5 = 110$$`
`$$\bar{x}=\frac{\sum{x_i}}{n} = \frac{90+95+100+105+110}{5}= 100$$`




---

# Measures of variation

* Central tendency captures e.g. the "center of gravity" in the data, however, we might also want to know something about its variation.

--

* The population variance:

`$$\sigma^2 =  \sum_{i=1}^{N}{(X_i-\mu)^2} / N$$`

--

* As the population parameters are unknown, we estimate them with our sample:

`$$s^2 =  \sum_{i=1}^{n}{(x_i-\bar{x})^2} / n-1$$`

---

# Variance

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; `\(x_i\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(\bar{x}\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(x_i-\bar{x}\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\((x_i-\bar{x})^2\)` &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 105.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 105.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 103.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 105.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.7 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 102.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 105.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.8 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 110.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 105.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -5.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27.1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 108.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 105.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -3.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10.4 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 100.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 105.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20.4 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 104.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 105.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 100.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 105.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 25.2 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 106.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 105.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.7 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 109.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 105.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 21.1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

Sum of deviations `\(=\sum{x_i-\bar{x}} = 0\)`.  
Sum of the square deviations `\(=\sum{(x_i-\bar{x})^2} = 828\)`  
Average square deviations = `\(\frac{\sum{(x_i-\bar{x})^2}}{n-1} = 92 = variance\)`  

---

# Variance and the standard deviation

* The variance is the average squared deviation from the mean

--

* The standard deviation is the square root of the variance, thus on the same scale as the mean

`$$\sqrt{\frac{\sum{(x_i-\bar{x})^2}}{n-1}} = SD$$`
--

* The standard error ...

`$$\frac{SD}{\sqrt{n}}=SE$$`

... is the estimate of the variation in the sampling distribution

---

&lt;img src="lecture-2020-1-inference1_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

---

# Sampling distributions

--

* The variation of a distribution of averages is affected by the sample size, the normal distribution does not account for sample sizes

--

* This variation can be estimated from samples using the standard error.

--

* As with proportions, the _**sample standard error**_ is an estimator of the **standard deviation of the sampling distribution**!

---

# Sampling distributions

&lt;img src="lecture-2020-1-inference1_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;

---

# Hypothesis testing 

--

* Based on the estimate of the sampling distribution we can device a test, to test if an average value exists within specified range. 

--

* 95% of all values lies within `\(\pm 1.96\times \sigma\)` from the mean in a normal distribution, this leaves us with an uncertainty of 5%.

--

* However, due to problems with *proving a theory or hypothesis*, we instead test against a null-hypothesis. Thus we try to *disprove the hypothesis*.

--

* The null hypothesis `\(H_0\)` is constructed to contain scenarios not covered by the alternative hypothesis `\(H_A\)`

---

# Hypothesis tests - a two sample scenario

--

* The null hypothesis is that the mean of group 1 is similar to group 2

`$$H_0: \mu_1 - \mu_2 = 0$$`

--

* To reject this hypothesis, we need to show that

`$$\mu_1 - \mu_2 \neq 0$$`

--

* We want to do this with some specified uncertainty, usually 5%

--

* We can calculate a 95% confidence interval of the difference 

---

# A 95% confidence interval for small samples

Upper bound: `$$\bar{x} + t_{0.975} \times SE$$`
Lower bound: `$$\bar{x} - t_{0.975} \times SE$$`

--

* `\(\bar{x}\)` is the difference in means between groups.

* The standard error ( `\(SE\)` ) estimates the standard deviation of the sampling distribution

* `\(t_{0.975}\)` represents the area under probability distribution curve containing 95% of all values.

* The `\(t\)`-distribution is used instead of the normal distribution since it can take sample-size into account. 

---

# The t-distribution

&lt;img src="lecture-2020-1-inference1_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;

---

# A two sample case

&lt;img src="lecture-2020-1-inference1_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;

---

# A 95% confidence of the difference in means

* Two groups are compared, the `\(H_0\)` is that there is no difference between the groups:

`$$H_0: \mu_1 = \mu_2$$`

--

* The difference between the groups are estimated to `\(\mu_2 - \mu_1 =\)` 0.97

--

* The 95% confidence interval is

 `$$m_2 - m_1 \pm t_{0.975} \times SE(m_2 - m_1)$$` 

--

where the `$$SE(m_2 - m_1)$$` is the &lt;span style="color:red"&gt;standard error of the difference&lt;/span&gt;. 

`$$0.97 \pm 2 \times 0.28$$`

---

# Summary

* We can *estimate* population *parameters* using a **random** sample from the population

--

* The calculated sample standard error is an estimate of the standard deviation of a sampling distribution

--

* Using a *probability density function* like the `\(t\)`- or `\(z\)`-distribution, we can estimate a range a plausible values of a population parameter (e.g. mean).

--

* We can test if a estimated interval contains the null hypothesis, if not we can reject `\(H_0\)`.



---

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

The chakra comes from [remark.js](https://remarkjs.com), [**knitr**](https://yihui.org/knitr), and [R Markdown](https://rmarkdown.rstudio.com).
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
